{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file=r'/Users/lalitsachan/Dropbox/0.0 Data/census_income.csv'\n",
    "\n",
    "cd= pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' >50K'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd['Y'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd['Y']=(cd['Y']==' >50K').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>education.num</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10th</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>933</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11th</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12th</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>433</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st-4th</th>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5th-6th</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7th-8th</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>646</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9th</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Assoc-acdm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1067</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Assoc-voc</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1382</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bachelors</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5355</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doctorate</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HS-grad</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10501</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Masters</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1723</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Preschool</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prof-school</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Some-college</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7291</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "education.num  1    2    3    4    5    6     7    8      9     10    11  \\\n",
       "education                                                                  \n",
       " 10th           0    0    0    0    0  933     0    0      0     0     0   \n",
       " 11th           0    0    0    0    0    0  1175    0      0     0     0   \n",
       " 12th           0    0    0    0    0    0     0  433      0     0     0   \n",
       " 1st-4th        0  168    0    0    0    0     0    0      0     0     0   \n",
       " 5th-6th        0    0  333    0    0    0     0    0      0     0     0   \n",
       " 7th-8th        0    0    0  646    0    0     0    0      0     0     0   \n",
       " 9th            0    0    0    0  514    0     0    0      0     0     0   \n",
       " Assoc-acdm     0    0    0    0    0    0     0    0      0     0     0   \n",
       " Assoc-voc      0    0    0    0    0    0     0    0      0     0  1382   \n",
       " Bachelors      0    0    0    0    0    0     0    0      0     0     0   \n",
       " Doctorate      0    0    0    0    0    0     0    0      0     0     0   \n",
       " HS-grad        0    0    0    0    0    0     0    0  10501     0     0   \n",
       " Masters        0    0    0    0    0    0     0    0      0     0     0   \n",
       " Preschool     51    0    0    0    0    0     0    0      0     0     0   \n",
       " Prof-school    0    0    0    0    0    0     0    0      0     0     0   \n",
       " Some-college   0    0    0    0    0    0     0    0      0  7291     0   \n",
       "\n",
       "education.num    12    13    14   15   16  \n",
       "education                                  \n",
       " 10th             0     0     0    0    0  \n",
       " 11th             0     0     0    0    0  \n",
       " 12th             0     0     0    0    0  \n",
       " 1st-4th          0     0     0    0    0  \n",
       " 5th-6th          0     0     0    0    0  \n",
       " 7th-8th          0     0     0    0    0  \n",
       " 9th              0     0     0    0    0  \n",
       " Assoc-acdm    1067     0     0    0    0  \n",
       " Assoc-voc        0     0     0    0    0  \n",
       " Bachelors        0  5355     0    0    0  \n",
       " Doctorate        0     0     0    0  413  \n",
       " HS-grad          0     0     0    0    0  \n",
       " Masters          0     0  1723    0    0  \n",
       " Preschool        0     0     0    0    0  \n",
       " Prof-school      0     0     0  576    0  \n",
       " Some-college     0     0     0    0    0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(cd['education'],cd['education.num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del cd['education']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt  education.num       marital.status  \\\n",
       "0   39          State-gov   77516             13        Never-married   \n",
       "1   50   Self-emp-not-inc   83311             13   Married-civ-spouse   \n",
       "2   38            Private  215646              9             Divorced   \n",
       "3   53            Private  234721              7   Married-civ-spouse   \n",
       "4   28            Private  338409             13   Married-civ-spouse   \n",
       "\n",
       "           occupation    relationship    race      sex  capital.gain  \\\n",
       "0        Adm-clerical   Not-in-family   White     Male          2174   \n",
       "1     Exec-managerial         Husband   White     Male             0   \n",
       "2   Handlers-cleaners   Not-in-family   White     Male             0   \n",
       "3   Handlers-cleaners         Husband   Black     Male             0   \n",
       "4      Prof-specialty            Wife   Black   Female             0   \n",
       "\n",
       "   capital.loss  hours.per.week  native.country  Y  \n",
       "0             0              40   United-States  0  \n",
       "1             0              13   United-States  0  \n",
       "2             0              40   United-States  0  \n",
       "3             0              40   United-States  0  \n",
       "4             0              40            Cuba  0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workclass          9\n",
       "marital.status     7\n",
       "occupation        15\n",
       "relationship       6\n",
       "race               5\n",
       "sex                2\n",
       "native.country    42\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd.select_dtypes(['object']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Prof-specialty       4140\n",
       " Craft-repair         4099\n",
       " Exec-managerial      4066\n",
       " Adm-clerical         3770\n",
       " Sales                3650\n",
       " Other-service        3295\n",
       " Machine-op-inspct    2002\n",
       " ?                    1843\n",
       " Transport-moving     1597\n",
       " Handlers-cleaners    1370\n",
       " Farming-fishing       994\n",
       " Tech-support          928\n",
       " Protective-serv       649\n",
       " Priv-house-serv       149\n",
       " Armed-Forces            9\n",
       "Name: occupation, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd['occupation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " United-States                 29170\n",
       " Mexico                          643\n",
       " ?                               583\n",
       " Philippines                     198\n",
       " Germany                         137\n",
       " Canada                          121\n",
       " Puerto-Rico                     114\n",
       " El-Salvador                     106\n",
       " India                           100\n",
       " Cuba                             95\n",
       " England                          90\n",
       " Jamaica                          81\n",
       " South                            80\n",
       " China                            75\n",
       " Italy                            73\n",
       " Dominican-Republic               70\n",
       " Vietnam                          67\n",
       " Guatemala                        64\n",
       " Japan                            62\n",
       " Poland                           60\n",
       " Columbia                         59\n",
       " Taiwan                           51\n",
       " Haiti                            44\n",
       " Iran                             43\n",
       " Portugal                         37\n",
       " Nicaragua                        34\n",
       " Peru                             31\n",
       " Greece                           29\n",
       " France                           29\n",
       " Ecuador                          28\n",
       " Ireland                          24\n",
       " Hong                             20\n",
       " Cambodia                         19\n",
       " Trinadad&Tobago                  19\n",
       " Thailand                         18\n",
       " Laos                             18\n",
       " Yugoslavia                       16\n",
       " Outlying-US(Guam-USVI-etc)       14\n",
       " Honduras                         13\n",
       " Hungary                          13\n",
       " Scotland                         12\n",
       " Holand-Netherlands                1\n",
       "Name: native.country, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd['native.country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols=cd.select_dtypes(['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['workclass', 'marital.status', 'occupation', 'relationship', 'race',\n",
       "       'sex', 'native.country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workclass\n",
      "marital.status\n",
      "occupation\n",
      "relationship\n",
      "race\n",
      "sex\n",
      "native.country\n"
     ]
    }
   ],
   "source": [
    "for col in cat_cols:\n",
    "    freqs=cd[col].value_counts()\n",
    "    k=freqs.index[freqs>99][:-1]\n",
    "    for cat in k:\n",
    "        name=col+'_'+cat\n",
    "        cd[name]=(cd[col]==cat).astype(int)\n",
    "    del cd[col]\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 49)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=cd.drop(['Y'],axis=1)\n",
    "y_train=cd['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MLPClassifier in module sklearn.neural_network.multilayer_perceptron:\n",
      "\n",
      "class MLPClassifier(BaseMultilayerPerceptron, sklearn.base.ClassifierMixin)\n",
      " |  Multi-layer Perceptron classifier.\n",
      " |  \n",
      " |  This model optimizes the log-loss function using LBFGS or stochastic\n",
      " |  gradient descent.\n",
      " |  \n",
      " |  .. versionadded:: 0.18\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n",
      " |      The ith element represents the number of neurons in the ith\n",
      " |      hidden layer.\n",
      " |  \n",
      " |  activation : {'identity', 'logistic', 'tanh', 'relu'}, default 'relu'\n",
      " |      Activation function for the hidden layer.\n",
      " |  \n",
      " |      - 'identity', no-op activation, useful to implement linear bottleneck,\n",
      " |        returns f(x) = x\n",
      " |  \n",
      " |      - 'logistic', the logistic sigmoid function,\n",
      " |        returns f(x) = 1 / (1 + exp(-x)).\n",
      " |  \n",
      " |      - 'tanh', the hyperbolic tan function,\n",
      " |        returns f(x) = tanh(x).\n",
      " |  \n",
      " |      - 'relu', the rectified linear unit function,\n",
      " |        returns f(x) = max(0, x)\n",
      " |  \n",
      " |  solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'\n",
      " |      The solver for weight optimization.\n",
      " |  \n",
      " |      - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
      " |  \n",
      " |      - 'sgd' refers to stochastic gradient descent.\n",
      " |  \n",
      " |      - 'adam' refers to a stochastic gradient-based optimizer proposed\n",
      " |        by Kingma, Diederik, and Jimmy Ba\n",
      " |  \n",
      " |      Note: The default solver 'adam' works pretty well on relatively\n",
      " |      large datasets (with thousands of training samples or more) in terms of\n",
      " |      both training time and validation score.\n",
      " |      For small datasets, however, 'lbfgs' can converge faster and perform\n",
      " |      better.\n",
      " |  \n",
      " |  alpha : float, optional, default 0.0001\n",
      " |      L2 penalty (regularization term) parameter.\n",
      " |  \n",
      " |  batch_size : int, optional, default 'auto'\n",
      " |      Size of minibatches for stochastic optimizers.\n",
      " |      If the solver is 'lbfgs', the classifier will not use minibatch.\n",
      " |      When set to \"auto\", `batch_size=min(200, n_samples)`\n",
      " |  \n",
      " |  learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'\n",
      " |      Learning rate schedule for weight updates.\n",
      " |  \n",
      " |      - 'constant' is a constant learning rate given by\n",
      " |        'learning_rate_init'.\n",
      " |  \n",
      " |      - 'invscaling' gradually decreases the learning rate ``learning_rate_``\n",
      " |        at each time step 't' using an inverse scaling exponent of 'power_t'.\n",
      " |        effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
      " |  \n",
      " |      - 'adaptive' keeps the learning rate constant to\n",
      " |        'learning_rate_init' as long as training loss keeps decreasing.\n",
      " |        Each time two consecutive epochs fail to decrease training loss by at\n",
      " |        least tol, or fail to increase validation score by at least tol if\n",
      " |        'early_stopping' is on, the current learning rate is divided by 5.\n",
      " |  \n",
      " |      Only used when ``solver='sgd'``.\n",
      " |  \n",
      " |  learning_rate_init : double, optional, default 0.001\n",
      " |      The initial learning rate used. It controls the step-size\n",
      " |      in updating the weights. Only used when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  power_t : double, optional, default 0.5\n",
      " |      The exponent for inverse scaling learning rate.\n",
      " |      It is used in updating effective learning rate when the learning_rate\n",
      " |      is set to 'invscaling'. Only used when solver='sgd'.\n",
      " |  \n",
      " |  max_iter : int, optional, default 200\n",
      " |      Maximum number of iterations. The solver iterates until convergence\n",
      " |      (determined by 'tol') or this number of iterations. For stochastic\n",
      " |      solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
      " |      (how many times each data point will be used), not the number of\n",
      " |      gradient steps.\n",
      " |  \n",
      " |  shuffle : bool, optional, default True\n",
      " |      Whether to shuffle samples in each iteration. Only used when\n",
      " |      solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  tol : float, optional, default 1e-4\n",
      " |      Tolerance for the optimization. When the loss or score is not improving\n",
      " |      by at least tol for two consecutive iterations, unless `learning_rate`\n",
      " |      is set to 'adaptive', convergence is considered to be reached and\n",
      " |      training stops.\n",
      " |  \n",
      " |  verbose : bool, optional, default False\n",
      " |      Whether to print progress messages to stdout.\n",
      " |  \n",
      " |  warm_start : bool, optional, default False\n",
      " |      When set to True, reuse the solution of the previous\n",
      " |      call to fit as initialization, otherwise, just erase the\n",
      " |      previous solution.\n",
      " |  \n",
      " |  momentum : float, default 0.9\n",
      " |      Momentum for gradient descent update. Should be between 0 and 1. Only\n",
      " |      used when solver='sgd'.\n",
      " |  \n",
      " |  nesterovs_momentum : boolean, default True\n",
      " |      Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
      " |      momentum > 0.\n",
      " |  \n",
      " |  early_stopping : bool, default False\n",
      " |      Whether to use early stopping to terminate training when validation\n",
      " |      score is not improving. If set to true, it will automatically set\n",
      " |      aside 10% of training data as validation and terminate training when\n",
      " |      validation score is not improving by at least tol for two consecutive\n",
      " |      epochs.\n",
      " |      Only effective when solver='sgd' or 'adam'\n",
      " |  \n",
      " |  validation_fraction : float, optional, default 0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if early_stopping is True\n",
      " |  \n",
      " |  beta_1 : float, optional, default 0.9\n",
      " |      Exponential decay rate for estimates of first moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'\n",
      " |  \n",
      " |  beta_2 : float, optional, default 0.999\n",
      " |      Exponential decay rate for estimates of second moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'\n",
      " |  \n",
      " |  epsilon : float, optional, default 1e-8\n",
      " |      Value for numerical stability in adam. Only used when solver='adam'\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : array or list of array of shape (n_classes,)\n",
      " |      Class labels for each output.\n",
      " |  \n",
      " |  loss_ : float\n",
      " |      The current loss computed with the loss function.\n",
      " |  \n",
      " |  coefs_ : list, length n_layers - 1\n",
      " |      The ith element in the list represents the weight matrix corresponding\n",
      " |      to layer i.\n",
      " |  \n",
      " |  intercepts_ : list, length n_layers - 1\n",
      " |      The ith element in the list represents the bias vector corresponding to\n",
      " |      layer i + 1.\n",
      " |  \n",
      " |  n_iter_ : int,\n",
      " |      The number of iterations the solver has ran.\n",
      " |  \n",
      " |  n_layers_ : int\n",
      " |      Number of layers.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      Number of outputs.\n",
      " |  \n",
      " |  out_activation_ : string\n",
      " |      Name of the output activation function.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  MLPClassifier trains iteratively since at each time step\n",
      " |  the partial derivatives of the loss function with respect to the model\n",
      " |  parameters are computed to update the parameters.\n",
      " |  \n",
      " |  It can also have a regularization term added to the loss function\n",
      " |  that shrinks model parameters to prevent overfitting.\n",
      " |  \n",
      " |  This implementation works with data represented as dense numpy arrays or\n",
      " |  sparse scipy arrays of floating point values.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  Hinton, Geoffrey E.\n",
      " |      \"Connectionist learning procedures.\" Artificial intelligence 40.1\n",
      " |      (1989): 185-234.\n",
      " |  \n",
      " |  Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n",
      " |      training deep feedforward neural networks.\" International Conference\n",
      " |      on Artificial Intelligence and Statistics. 2010.\n",
      " |  \n",
      " |  He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n",
      " |      performance on imagenet classification.\" arXiv preprint\n",
      " |      arXiv:1502.01852 (2015).\n",
      " |  \n",
      " |  Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n",
      " |      optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MLPClassifier\n",
      " |      BaseMultilayerPerceptron\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model to data matrix X and target(s) y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns a trained MLP model.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the multi-layer perceptron classifier\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like, shape (n_samples,) or (n_samples, n_classes)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return the log of probability estimates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      log_y_prob : array-like, shape (n_samples, n_classes)\n",
      " |          The predicted log-probability of the sample for each class\n",
      " |          in the model, where classes are ordered as they are in\n",
      " |          `self.classes_`. Equivalent to log(predict_proba(X))\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_prob : array-like, shape (n_samples, n_classes)\n",
      " |          The predicted probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in `self.classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  partial_fit\n",
      " |      Fit the model to data matrix X and target y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target values.\n",
      " |      \n",
      " |      classes : array, shape (n_classes)\n",
      " |          Classes across all calls to partial_fit.\n",
      " |          Can be obtained via `np.unique(y_all)`, where y_all is the\n",
      " |          target vector of the entire dataset.\n",
      " |          This argument is required for the first call to partial_fit\n",
      " |          and can be omitted in the subsequent calls.\n",
      " |          Note that y doesn't need to contain all labels in `classes`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns a trained MLP model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MLPClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters={\n",
    "'learning_rate': [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "'hidden_layer_sizes': [(5,10,5),(20,10),(10,20)],\n",
    "'alpha': [0.3,.1,.01],\n",
    "'activation': [\"logistic\", \"relu\", \"tanh\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf=MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search=RandomizedSearchCV(clf,n_iter=5,cv=10,param_distributions=parameters,\n",
    "                                 scoring='roc_auc',random_state=2,n_jobs=-1,verbose=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "[CV] learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic \n",
      "[CV] learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic \n",
      "[CV] learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic \n",
      "[CV] learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic \n",
      "[CV]  learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic, score=0.5017093356419929, total=   2.1s\n",
      "[CV] learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic \n",
      "[CV]  learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic, score=0.5006377551020408, total=   2.0s\n",
      "[CV] learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic, score=0.5001406053926425, total=   3.0s\n",
      "[CV] learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    3.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic, score=0.5, total=   3.2s\n",
      "[CV] learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    3.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic, score=0.49702509948154017, total=   1.7s\n",
      "[CV]  learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic, score=0.5102458762631266, total=   1.7s\n",
      "[CV] learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic \n",
      "[CV]  learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic, score=0.5012360374975232, total=   2.4s\n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu \n",
      "[CV]  learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic, score=0.505624989680338, total=   2.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    6.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu \n",
      "[CV]  learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic, score=0.5, total=   2.4s\n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=invscaling, hidden_layer_sizes=(10, 20), alpha=0.01, activation=logistic, score=0.5161920654184003, total=   3.0s\n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    7.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu, score=0.678434904046338, total=   1.7s\n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:    8.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu, score=0.5919262577603858, total=   2.3s\n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu \n",
      "[CV]  learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu, score=0.6235869803018295, total=   1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    9.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu \n",
      "[CV]  learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu, score=0.530997684267882, total=   3.8s\n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   10.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu, score=0.5752811591869758, total=   2.8s\n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:   10.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu, score=0.6642655393468068, total=   2.3s\n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   11.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu, score=0.6336527784657552, total=   1.8s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   12.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu, score=0.6426814506142262, total=   2.4s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   12.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu, score=0.5697338249620236, total=   3.5s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:   13.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=constant, hidden_layer_sizes=(20, 10), alpha=0.3, activation=relu, score=0.6687543342579751, total=   1.8s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:   13.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu, score=0.5951524420447791, total=   1.7s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:   14.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu, score=0.5744656479096493, total=   1.5s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:   15.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu, score=0.5006369426751592, total=   5.1s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:   17.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu, score=0.6455959811108909, total=   5.1s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   20.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu, score=0.6110403354302887, total=   2.3s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   20.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu, score=0.6445155125156858, total=   2.4s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   22.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu, score=0.6393347842777888, total=   8.7s\n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:   24.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu, score=0.5741189072716465, total=  12.0s\n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:   25.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu, score=0.6288058912885542, total=   3.1s\n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh \n",
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.3, activation=relu, score=0.5108418367346939, total=   5.5s\n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:   25.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh, score=0.4990929017237963, total=   1.5s\n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh \n",
      "[CV]  learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh, score=0.5001424113334655, total=   1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:   27.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh, score=0.5076407354729661, total=   2.8s\n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh \n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh \n",
      "[CV]  learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh, score=0.4987438391618783, total=   1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:   27.3s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   27.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   27.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh, score=0.5150228477313256, total=   1.3s\n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:   28.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh, score=0.5307123456178587, total=   1.7s\n",
      "[CV] learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:   29.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh, score=0.5279944046793476, total=   2.6s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   29.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh, score=0.49892675516808666, total=   1.4s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:   30.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh, score=0.5214765062578429, total=   3.0s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:   30.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=constant, hidden_layer_sizes=(10, 20), alpha=0.01, activation=tanh, score=0.5485697464665479, total=   1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:   30.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh \n",
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh, score=0.4958502059804505, total=   1.3s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:   31.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh, score=0.5261545874301733, total=   2.5s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   32.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh, score=0.502110112855822, total=   1.9s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed:   33.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh, score=0.5, total=   2.5s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh \n",
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh, score=0.5006377551020408, total=   1.8s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh \n",
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh, score=0.5108199074532725, total=   2.7s\n",
      "[CV] learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  46 out of  50 | elapsed:   35.4s remaining:    3.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh, score=0.5308862319199524, total=   2.2s\n",
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh, score=0.5019132653061225, total=   2.0s\n",
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh, score=0.5694428104979856, total=   2.9s\n",
      "[CV]  learning_rate=adaptive, hidden_layer_sizes=(5, 10, 5), alpha=0.01, activation=tanh, score=0.4987456451027013, total=   1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   36.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=5, n_jobs=-1,\n",
       "          param_distributions={'learning_rate': ['constant', 'invscaling', 'adaptive'], 'hidden_layer_sizes': [(5, 10, 5), (20, 10), (10, 20)], 'alpha': [0.3, 0.1, 0.01], 'activation': ['logistic', 'relu', 'tanh']},\n",
       "          pre_dispatch='2*n_jobs', random_state=2, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=20)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.3, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(20, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.5f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.618 (std: 0.04654)\n",
      "Parameters: {'learning_rate': 'constant', 'hidden_layer_sizes': (20, 10), 'alpha': 0.3, 'activation': 'relu'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.592 (std: 0.05016)\n",
      "Parameters: {'learning_rate': 'adaptive', 'hidden_layer_sizes': (5, 10, 5), 'alpha': 0.3, 'activation': 'relu'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.515 (std: 0.01623)\n",
      "Parameters: {'learning_rate': 'constant', 'hidden_layer_sizes': (10, 20), 'alpha': 0.01, 'activation': 'tanh'}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.514 (std: 0.02177)\n",
      "Parameters: {'learning_rate': 'adaptive', 'hidden_layer_sizes': (5, 10, 5), 'alpha': 0.01, 'activation': 'tanh'}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: 0.503 (std: 0.00552)\n",
      "Parameters: {'learning_rate': 'invscaling', 'hidden_layer_sizes': (10, 20), 'alpha': 0.01, 'activation': 'logistic'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(random_search.cv_results_,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp=random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.3, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(20, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
